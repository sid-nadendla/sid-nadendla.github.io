<!DOCTYPE html>
<html lang="en">

<!-- Style for images -->
<style>
    .grid { 
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(500px, 1fr));
        grid-gap: 20px;
        align-items: stretch;
        justify-items: center;
        }
    .grid img {
        border: 1px solid #ccc;
        box-shadow: 4px 4px 6px 0px  rgba(0,0,0,0.3);
        max-width: 75%;
        text-align: center;
    }
    </style>

<head>
    <meta charset="utf-8">
    <title>Research</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="CPHS Lab, Home, Missouri University of Science and Technology">
    <meta name="author" content="">
    <!-- Le styles -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="css/theme.css" rel="stylesheet">

    <script src='js/jquery-1.9.1.min.js' type='text/javascript'></script>
    <script type='text/javascript'>
        $(document).ready(function() {
            $("#header").load("header.html");
        });
    </script>

    <script type='text/javascript'>
        $(document).ready(function() {
            $("#footer").load("footer.html");
        });
    </script>

</head>

<body>
    <div class="container">
        <div id="header"></div>
        <hr>
        <div class="row-fluid">
            <section id="nano">
                <div class="page-header">
                    <h2>Fair Kidney Placement</h2>
                    
                    <img src="images/research/machine-bias.png" class="float-right" alt="fairness" style="width: 30%;" />
                    
                    <h4>Background and Objectives</h4>
                    <p align='justify'>

                    </p>

                    <h5>Supporting Agencies</h5>
                    <img src="images/research/FundingAgencies/nsf_logo.jpeg" class="float-left" alt="NSF" style="width: 30%;" />
                    <!-- <ul>
                        <li>
                            C. Canfield (PI), C. Dagli, D. Shank, V. S. S. Nadendla, 
                            "Collaborative Research: FW-HTF-R: Embedding Preferences in Adaptable Artificial Intelligence Decision Support for Transplant Healthcare to Reduce Kidney Discard," 
                            Future-of-Work: Human-Technology Frontier, National Science Foundation.
                        </li>
                    </ul> -->

                    <hr>

                    <h4><a href="">Thrust 1: Learning and Aggregating Fairness Preferences of Diverse Stakeholders</a></h4>
                    <p align='justify'></p>
                    <h5>References</h5>
                    <ul>
                        <li>CHIL'2024</li>
                    </ul>

                    <hr>

                    <h4><a href="">Thrust 2: Fairness Through Disagreements</a></h4>
                    <p align='justify'></p>
                    <h5>References</h5>
                    <ul>
                        <li>BIAS'2023</li>
                    </ul>

                    <hr>
                    
                    <h4><a href="">Thrust 3: Non-Comparative Fairness for Human-Auditing</a></h4>
                    <p align='justify'>
                        Bias evaluation in machine-learning based services (MLS) based on traditional algorithmic fairness notions that rely on comparative principles is practically difficult, making it necessary to rely on human auditor feedback. However, in spite of taking
                        rigorous training on various comparative fairness notions, human auditors are known to disagree on various aspects of fairness notions in practice, making it difficult to collect reliable feedback.With the advancements
                        of algorithmic decision making, biases and unfair treatment have also increased which affects human lives significantly. Many fair notions have been introduced in the past few years in order to eliminate the biases.
                        But, these notions of fairness do not satisfy individual preferences and does not meet the expectations of society. We introduce a new notion of fairness called subjective fairness where a trusted auditor evaluates
                        the efficiency of a classifier by employing an arbitrary fairness rule. We also design a post-processing approach to improve subjective fairness using a randomization scheme over the outputs and prove that, the probability
                        of improving fairness is independent of the randomization scheme. Our work offers a paradigm shift to the domain of algorithmic fairness via proposing a new fairness notion based on the principle of non-comparative
                        justice. In contrary to traditional fairness notions where the outcomes of two individuals/groups are compared, our proposed notion compares the MLS' outcome with a desired outcome for each input. This desired outcome
                        naturally describes a human auditor's expectation, and can be easily used to evaluate MLS on crowd-auditing platforms.
                    </p>
                    <h5>References</h5>
                    <ul>
                        <li>FAccTRec'2021</li>
                    </ul>
                </div>
            </section>
        </div>
    </div>

    <div id="footer"></div>
    <!-- Le javascript
         ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="js/jquery-1.9.1.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script>
        $(document).ready(function() {
            $(document.body).scrollspy({
                target: "#navparent"
            });
        });
    </script>
</body>

</html>
